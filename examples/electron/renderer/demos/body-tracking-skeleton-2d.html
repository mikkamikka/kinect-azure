<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Kinect Azure Example</title>
    <link rel="stylesheet" href="../assets/vendors/bootstrap-4.3.1-dist/css/bootstrap.css">
    <link rel="stylesheet" href="../assets/vendors/bootstrap-4.3.1-dist/css/docs.min.css">
  </head>
  <body class="container-fluid py-3">
    <div class="d-flex align-items-baseline justify-content-between">
        <h1 class="bd-title">Body Tracking (2D)</h1>
        <button onclick="require('electron').remote.getCurrentWebContents().openDevTools()">open dev tools</button>
      </div>
      <p>
        This demo shows the 2D Skeleton Information.
      </p>
    <canvas id="outputCanvas" class="img-fluid"></canvas>
    <div class="row">
      <div class="col col-auto">Renderer: <div id="statsRenderer"></div></div>
      <div class="col col-auto">Kinect: <div id="statsKinect"></div></div>
    </div>
    
    <script src="../assets/vendors/stats.min.js"></script>
    <script>
      {
        const statsRenderer = new Stats();
        statsRenderer.dom.style.cssText = '';
        document.getElementById('statsRenderer').appendChild( statsRenderer.dom );
        const statsKinect = new Stats();
        statsKinect.dom.style.cssText = '';
        document.getElementById('statsKinect').appendChild( statsKinect.dom );

        const KinectAzure = require('kinect-azure');
        const kinect = new KinectAzure();

        const $outputCanvas = document.getElementById('outputCanvas'),
          outputCtx = $outputCanvas.getContext('2d');
        let outputImageData, depthModeRange;

        const init = () => {
          startKinect();
          animate();
        };

        const startKinect = () => {
          if(kinect.open()) {

            kinect.startCameras({
              depth_mode: KinectAzure.K4A_DEPTH_MODE_NFOV_UNBINNED
            //   depth_mode: KinectAzure.K4A_DEPTH_MODE_WFOV_2X2BINNED 
            });
            depthModeRange = kinect.getDepthModeRange(KinectAzure.K4A_DEPTH_MODE_NFOV_UNBINNED);
            kinect.createTracker(
                {

                    // processing_mode: KinectAzure.K4ABT_TRACKER_PROCESSING_MODE_GPU_CUDA,
                    //processing_mode: KinectAzure.K4ABT_TRACKER_PROCESSING_MODE_GPU_DIRECTML,
                    processing_mode: KinectAzure.K4ABT_TRACKER_PROCESSING_MODE_GPU_TENSORRT

                    // model_path: "C:\\Program Files\\Azure Kinect Body Tracking SDK\\tools\\dnn_model_2_0_op11.onnx"
                    //model_path: "C:\\Program Files\\Azure Kinect Body Tracking SDK\\tools\\dnn_model_2_0_lite_op11.onnx"

                }
            );

            kinect.startListening((data) => {
              statsKinect.update();
              if (!outputImageData && data.depthImageFrame.width > 0) {
                $outputCanvas.width = data.depthImageFrame.width;
                $outputCanvas.height = data.depthImageFrame.height;
                outputImageData = outputCtx.createImageData($outputCanvas.width, $outputCanvas.height);
              }
              if (outputImageData) {
                renderDepthFrameAsGreyScale(outputCtx, outputImageData, data.depthImageFrame);
              }
              if (data.bodyFrame.bodies) {
                // render the skeleton joints on top of the depth feed
                outputCtx.save();
                data.bodyFrame.bodies.forEach(body => {
                  outputCtx.fillStyle = 'red';
                  body.skeleton.joints.forEach(joint => {
                    outputCtx.fillRect(joint.depthX, joint.depthY, 10, 10);
                  });
                  // draw the pelvis as a green square
                  const pelvis = body.skeleton.joints[KinectAzure.K4ABT_JOINT_PELVIS];
                  outputCtx.fillStyle = 'green';
                  outputCtx.fillRect(pelvis.depthX, pelvis.depthY, 10, 10);
                });
                outputCtx.restore();
              }
            });
          }
        };

        const renderDepthFrameAsGreyScale = (ctx, canvasImageData, imageFrame) => {
          const newPixelData = Buffer.from(imageFrame.imageData);
          const pixelArray = canvasImageData.data;
          let depthPixelIndex = 0;
          for (let i = 0; i < canvasImageData.data.length; i+=4) {
            const depthValue = newPixelData[depthPixelIndex+1] << 8 | newPixelData[depthPixelIndex];
            const normalizedValue = map(depthValue, depthModeRange.min, depthModeRange.max, 255, 0);
            pixelArray[i] = normalizedValue;
            pixelArray[i+1] = normalizedValue;
            pixelArray[i+2] = normalizedValue;
            pixelArray[i+3] = 0xff;
            depthPixelIndex += 2;
          }
          ctx.putImageData(canvasImageData, 0, 0);
        };

        const map = (value, inputMin, inputMax, outputMin, outputMax) => {
          return (value - inputMin) * (outputMax - outputMin) / (inputMax - inputMin) + outputMin;
        };

        const animate = () => {
          statsRenderer.update();
          requestAnimationFrame( animate );
        }

        // expose the kinect instance to the window object in this demo app to allow the parent window to close it between sessions
        window.kinect = kinect;
        init();
      }
    </script>
  </body>
</html>